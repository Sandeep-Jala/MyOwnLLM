{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "id": "0ybQk3bsXkkT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lExSq70zUucE"
      },
      "outputs": [],
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super(InputEmbeddings, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we do postional encoding"
      ],
      "metadata": {
        "id": "kRP136wWYimn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_length = max_seq_length\n",
        "        Pe = torch.zeros(max_seq_length, d_model)\n",
        "        Postions = torch.arange(0,max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        Pe[:,0::2] = torch.sin(Postions * div_term)\n",
        "        Pe[:,1::2] = torch.cos(Postions * div_term)\n",
        "        self.register_buffer('Pe', Pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "      return x + self.Pe[:, :x.size(1)]\n"
      ],
      "metadata": {
        "id": "nrHEeZXKYMBp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize layers\n",
        "embedding_layer = InputEmbeddings(1000, 6)\n",
        "positional_encoding = PositionalEncoding(6, 1000)\n",
        "\n",
        "# Example input\n",
        "input_ids = torch.randint(0, 1000, (1, 1000))  # shape (batch_size, seq_length)\n",
        "print(input_ids.shape)\n",
        "# Apply embedding\n",
        "embedded = embedding_layer(input_ids)  # shape (1, 5, 5)\n",
        "\n",
        "# Apply positional encoding\n",
        "output = positional_encoding(embedded)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuEV5LLOZpxn",
        "outputId": "4b0a43a5-7fea-4a0a-e7c3-91ed71abc1c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1000])\n",
            "tensor([[[ 0.3100,  3.6809,  1.5058,  1.7296,  1.6718, -0.3308],\n",
            "         [ 1.0574,  6.8504, -2.6296,  1.4777,  4.8250, -0.3278],\n",
            "         [ 4.3780, -0.7868,  3.2376,  4.3001,  2.9273,  1.0790],\n",
            "         ...,\n",
            "         [ 1.7624,  0.5264,  2.5049,  3.9940, -2.8293, -0.7895],\n",
            "         [ 1.3450,  0.6577, -2.2282,  0.8381,  4.8667, -2.7859],\n",
            "         [ 0.0357,  1.2243, -1.6068,  2.3189,  1.6121, -5.1969]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MultiHead Attention"
      ],
      "metadata": {
        "id": "QwDeKIg1GMs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model,max_seq_length):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.max_seq_length\n",
        "    self.head_dim = d_model // num_heads\n",
        "    self.query_linear = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.key_linear = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.value_linear = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.output_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    seq_length = x.size(1)\n",
        "    # we split the diemntions into multiple heads\n",
        "    x = x.reshape(batch_size, seq_length, self.num_heads, self.head_dim)\n",
        "    # Reshuffle in correct order for the attention\n",
        "    return x.permute(0, 2, 1, 3)\n",
        "\n",
        "  def compute_attention(self, query, key, value, mask=None):\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    return torch.matmul(attention_weights, value)\n",
        "\n",
        "  def combine_heads(self, x, batch_size):\n",
        "      seq_length = x.size(1)\n",
        "      # Combine heads back to (batch_size, seq_length, d_model)\n",
        "      x = x.permute(0, 2, 1, 3).contiguous()\n",
        "      return x.view(batch_size, -1, self.d_model)\n",
        "\n",
        "  def forward(self, query, key, value, mask=None):\n",
        "      batch_size = query.size(0)\n",
        "\n",
        "      # Build the forward pass\n",
        "      query = self.split_heads(self.query_linear(query), batch_size)\n",
        "      key = self.split_heads(self.key_linear(key), batch_size)\n",
        "      value = self.split_heads(self.value_linear(value), batch_size)\n",
        "\n",
        "      attention_weights = self.compute_attention(query, key, value, mask)\n",
        "      output = self.combine_heads(attention_weights, batch_size)\n",
        "      return self.output_linear(output)\n"
      ],
      "metadata": {
        "id": "IdRLxc4pCZUU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Model"
      ],
      "metadata": {
        "id": "fPTGNFArMoNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):"
      ],
      "metadata": {
        "id": "1TgjciKPMjnw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):"
      ],
      "metadata": {
        "id": "G0dsIIR5GtB0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}